# A Basic Project on Titanic Dataset

## Problem Statement: <br>
Titanic classification challenge on Kaggle. Given a dataset of a subset of the Titanic's passengers predict whether they will survive or not.

## Procedure: <br>
Below are provided the steps that were followed for this project. Each step and classifiers have their own document.

1. Data visualization: data analysis to understand missing values, data relations and usefulness of features
2. Preprocessing: with the knowledge acquired with the preceding step, apply preprocessing of data including dealing with missing values, drop unuseful features and build new features
3. Classifier: build classifiers based on the preprocessed data using a variety of techniques

## Classification Algorithm Used: <br>
XGBClassifier: XGBoost is short for “eXtreme Gradient Boosting.” The “eXtreme” refers to speed enhancements such as parallel computing and cache awareness that makes XGBoost approximately 10 times faster than traditional Gradient Boosting. In addition, XGBoost includes a unique split-finding algorithm to optimize trees, along with built-in regularization that reduces overfitting. Generally speaking, XGBoost is a faster, more accurate version of Gradient Boosting.

